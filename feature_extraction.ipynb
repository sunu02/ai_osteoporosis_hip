{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.applications import InceptionV3\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import GlobalAveragePooling2D, Input\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "import tensorflow as tf\n",
    "\n",
    "target_size = (299, 299)\n",
    "batch_size = 256\n",
    "epochs = 1000\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.0001\n",
    "\n",
    "work_dir = '/home/jarvis/workspace/ai_osteoporosis_hip'\n",
    "share_dir = '/sdb1/share/ai_osteoporosis_hip'\n",
    "crop_dir = os.path.join(share_dir, 'crop')\n",
    "xls_dir = os.path.join(share_dir, 'excel_files')\n",
    "xls_final = os.path.join(xls_dir, 'final.xlsx')\n",
    "\n",
    "conv_base = InceptionV3(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(target_size[0], target_size[1], 3))\n",
    "\n",
    "i = conv_base.input\n",
    "x = conv_base(i)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "model = Model(inputs=i, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 299, 299, 3)       0         \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 8, 8, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "=================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5, 5) \n",
    "\n",
    "# Augmentation parameters\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "#    iaa.Flipud(0.5),\n",
    "    iaa.Crop(percent=(0, 0.1)),\n",
    "    iaa.Multiply((0.9, 1.1)),\n",
    "    iaa.Affine(\n",
    "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, #{\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, \n",
    "        rotate=(-40, 40),\n",
    "        shear=(-20, 20),\n",
    "        order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "        cval=0,\n",
    "        mode='edge'#['constant', 'edge']\n",
    "    ),\n",
    "    iaa.OneOf([\n",
    "        iaa.GaussianBlur((0, 1.0)),\n",
    "#        iaa.AverageBlur(k=(2, 7)),\n",
    "        iaa.Sharpen(alpha=(0, 0.1), lightness=(0.75, 1.5))\n",
    "    ]),\n",
    "])\n",
    "\n",
    "\n",
    "def generate_df(xls_file):\n",
    "    df = pd.read_excel(xls_file, dtype={'patient_id':str})\n",
    "\n",
    "    d1 = df[df['split']==1]\n",
    "\n",
    "    columns_list = ['age', 'height', 'weight', 'bmi']\n",
    "    \n",
    "    for column_name in columns_list:\n",
    "        mean = d1[column_name].mean()\n",
    "        std = d1[column_name].std()\n",
    "        df[column_name] = df[column_name].apply(lambda x: (x - mean) / std)\n",
    "        \n",
    "    sex_to_code = {'M': 1, 'F':0}\n",
    "    df['sex_code'] = df['sex'].apply(lambda x: sex_to_code[x])    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def resample(df):\n",
    "    resampled_list = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        age = df.iloc[i]['age']\n",
    "        sex = df.iloc[i]['sex_code']\n",
    "        bmi = df.iloc[i]['bmi']\n",
    "        class2_1 = df.iloc[i]['class2_1']\n",
    "        split = df.iloc[i]['split']\n",
    "        file_name = df.iloc[i]['file_name']\n",
    "        if split in [2, 3]:\n",
    "            resampled_list.append((age, sex, bmi, class2_1, split, file_name))\n",
    "        elif class2_1 == 0:\n",
    "            resampled_list.append((age, sex, bmi, class2_1, split, file_name))\n",
    "        else:\n",
    "            resampled_list.append((age, sex, bmi, class2_1, split, file_name))\n",
    "            resampled_list.append((age, sex, bmi, class2_1, split, file_name))\n",
    "            resampled_list.append((age, sex, bmi, class2_1, split, file_name))\n",
    "    \n",
    "    random.shuffle(resampled_list)\n",
    "    df2 = pd.DataFrame(resampled_list, columns=['age', 'sex_code', 'bmi', 'class2_1', 'split', 'file_name'])\n",
    "                     \n",
    "    return df2\n",
    "\n",
    "def read_image(file_path, target_size, is_train):\n",
    "    img_gray = np.array(imageio.imread(file_path))\n",
    "    max_val = np.amax(img_gray)\n",
    "    img_gray = img_gray / max_val\n",
    "    img_resized = resize(img_gray, target_size)\n",
    "    img = np.dstack((img_resized, img_resized, img_resized))\n",
    "\n",
    "    if is_train:\n",
    "        #img = seq.augment_images(x)\n",
    "        #plt.imshow(img, cmap='gray')\n",
    "        #plt.show()\n",
    "        img = seq.augment_image(img)\n",
    "        #plt.imshow(img, cmap='gray')\n",
    "        #plt.show()        \n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_features(df, dir_name, target_size, batch_size, set_name):\n",
    "    split_dict = {'train':1, 'validation':2, 'test':3}\n",
    "    split = split_dict[set_name]\n",
    "    data = df[df['split']==split]\n",
    "    count = len(data)\n",
    "    \n",
    "    div = count // batch_size\n",
    "    res = count % batch_size\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    print('sample count=%d' % count)\n",
    "\n",
    "    if set_name == 'train':\n",
    "        features = np.zeros(shape=(count*5, 2048))\n",
    "        labels = np.zeros(shape=(count*5))\n",
    "        \n",
    "        for l in range(5):\n",
    "            i = 0\n",
    "            for n in range(div+1):\n",
    "                if n == div and res != 0: img_batch_size = res\n",
    "                else: img_batch_size = batch_size\n",
    "                img_batch = np.zeros(shape=(img_batch_size, target_size[0], target_size[1], 3))\n",
    "                label_batch = np.zeros(shape=(img_batch_size))\n",
    "                \n",
    "                for m in range(img_batch_size):\n",
    "                    file_name = data.iloc[i]['file_name']\n",
    "                    file_path = os.path.join(dir_name, file_name)\n",
    "                    class2_1 = data.iloc[i]['class2_1']\n",
    "\n",
    "                    img_batch[m] = read_image(file_path, target_size, is_train=True)\n",
    "                    label_batch[m] = class2_1\n",
    "                    i += 1\n",
    "                    print('Processing image %d...\\r' % i, end='')\n",
    "                    sys.stdout.flush()\n",
    "                features_batch = model.predict(img_batch)\n",
    "                features[count*l+n*batch_size : count*l+n*batch_size+img_batch_size] = features_batch            \n",
    "                labels[count*l+n*batch_size : count*l+n*batch_size+img_batch_size] = label_batch            \n",
    "                print('Filling batch %d~%d...' % (count*l+n*batch_size ,count*l+n*batch_size+img_batch_size))\n",
    "    else:\n",
    "        features = np.zeros(shape=(count, 2048))\n",
    "        labels = np.zeros(shape=(count))\n",
    "\n",
    "        for n in range(div+1):\n",
    "            if n == div and res != 0: img_batch_size = res\n",
    "            else: img_batch_size = batch_size\n",
    "            img_batch = np.zeros(shape=(img_batch_size, target_size[0], target_size[1], 3))\n",
    "\n",
    "            for m in range(img_batch_size):\n",
    "                file_name = data.iloc[i]['file_name']\n",
    "                file_path = os.path.join(dir_name, file_name)\n",
    "                class2_1 = data.iloc[i]['class2_1']\n",
    "\n",
    "                img_batch[m] = read_image(file_path, target_size, is_train=False)\n",
    "                labels[i] = class2_1\n",
    "                i += 1\n",
    "                print('Processing image %d...\\r' % i, end='')\n",
    "                sys.stdout.flush()\n",
    "            features_batch = model.predict(img_batch)\n",
    "            features[n*batch_size : n*batch_size+img_batch_size] = features_batch            \n",
    "    return features, labels\n",
    "\n",
    "def extract_train_features(df, dir_name, target_size, batch_size):\n",
    "    data = df[df['split']==1]\n",
    "    count = len(data)\n",
    "    \n",
    "    div = count // batch_size\n",
    "    res = count % batch_size\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    print('sample count=%d' % count)\n",
    "\n",
    "    features = np.zeros(shape=(count*5, 2048))\n",
    "    labels = np.zeros(shape=(count*5))\n",
    "\n",
    "    for n in range(div+1):\n",
    "        if n == div and res != 0: img_batch_size = res\n",
    "        else: img_batch_size = batch_size\n",
    "        img_batch = np.zeros(shape=(img_batch_size, target_size[0], target_size[1], 3))\n",
    "        label_batch = np.zeros(shape=(img_batch_size))\n",
    "\n",
    "        for m in range(img_batch_size):\n",
    "            file_name = data.iloc[i]['file_name']\n",
    "            file_path = os.path.join(dir_name, file_name)\n",
    "            class2_1 = data.iloc[i]['class2_1']\n",
    "\n",
    "            img_batch[m] = read_image(file_path, target_size, is_train=True)\n",
    "            label_batch[m] = class2_1\n",
    "            i += 1\n",
    "            print('Processing image %d...\\r' % i, end='')\n",
    "            sys.stdout.flush()\n",
    "        features_batch = model.predict(img_batch)\n",
    "        features[n*batch_size : n*batch_size+img_batch_size] = features_batch            \n",
    "        labels[n*batch_size : n*batch_size+img_batch_size] = label_batch\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = generate_df(xls_final)\n",
    "df_resample = resample(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample count=13052\n",
      "Processing image 4...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarvis/anaconda3/envs/naesa1.8/lib/python3.5/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/jarvis/anaconda3/envs/naesa1.8/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling batch 0~256....\n",
      "Filling batch 256~512...\n",
      "Filling batch 512~768...\n",
      "Filling batch 768~1024...\n",
      "Filling batch 1024~1280...\n",
      "Filling batch 1280~1536...\n",
      "Filling batch 1536~1792...\n",
      "Filling batch 1792~2048...\n",
      "Filling batch 2048~2304...\n",
      "Filling batch 2304~2560...\n",
      "Filling batch 2560~2816...\n",
      "Filling batch 2816~3072...\n",
      "Filling batch 3072~3328...\n",
      "Filling batch 3328~3584...\n",
      "Filling batch 3584~3840...\n",
      "Filling batch 3840~4096...\n",
      "Filling batch 4096~4352...\n",
      "Filling batch 4352~4608...\n",
      "Filling batch 4608~4864...\n",
      "Filling batch 4864~5120...\n",
      "Filling batch 5120~5376...\n",
      "Filling batch 5376~5632...\n",
      "Filling batch 5632~5888...\n",
      "Filling batch 5888~6144...\n",
      "Filling batch 6144~6400...\n",
      "Filling batch 6400~6656...\n",
      "Filling batch 6656~6912...\n",
      "Filling batch 6912~7168...\n",
      "Filling batch 7168~7424...\n",
      "Filling batch 7424~7680...\n",
      "Filling batch 7680~7936...\n",
      "Filling batch 7936~8192...\n",
      "Filling batch 8192~8448...\n",
      "Filling batch 8448~8704...\n",
      "Filling batch 8704~8960...\n",
      "Filling batch 8960~9216...\n",
      "Filling batch 9216~9472...\n",
      "Filling batch 9472~9728...\n",
      "Filling batch 9728~9984...\n",
      "Filling batch 9984~10240...\n",
      "Filling batch 10240~10496...\n",
      "Filling batch 10496~10752...\n",
      "Filling batch 10752~11008...\n",
      "Filling batch 11008~11264...\n",
      "Filling batch 11264~11520...\n",
      "Filling batch 11520~11776...\n",
      "Filling batch 11776~12032...\n",
      "Filling batch 12032~12288...\n",
      "Filling batch 12288~12544...\n",
      "Filling batch 12544~12800...\n",
      "Filling batch 12800~13052...\n",
      "Filling batch 13052~13308...\n",
      "Filling batch 13308~13564...\n",
      "Filling batch 13564~13820...\n",
      "Filling batch 13820~14076...\n",
      "Filling batch 14076~14332...\n",
      "Filling batch 14332~14588...\n",
      "Filling batch 14588~14844...\n",
      "Filling batch 14844~15100...\n",
      "Filling batch 15100~15356...\n",
      "Filling batch 15356~15612...\n",
      "Filling batch 15612~15868...\n",
      "Filling batch 15868~16124...\n",
      "Filling batch 16124~16380...\n",
      "Filling batch 16380~16636...\n",
      "Filling batch 16636~16892...\n",
      "Filling batch 16892~17148...\n",
      "Filling batch 17148~17404...\n",
      "Filling batch 17404~17660...\n",
      "Filling batch 17660~17916...\n",
      "Filling batch 17916~18172...\n",
      "Filling batch 18172~18428...\n",
      "Filling batch 18428~18684...\n",
      "Filling batch 18684~18940...\n",
      "Filling batch 18940~19196...\n",
      "Filling batch 19196~19452...\n",
      "Filling batch 19452~19708...\n",
      "Filling batch 19708~19964...\n",
      "Filling batch 19964~20220...\n",
      "Filling batch 20220~20476...\n",
      "Filling batch 20476~20732...\n",
      "Filling batch 20732~20988...\n",
      "Filling batch 20988~21244...\n",
      "Filling batch 21244~21500...\n",
      "Filling batch 21500~21756...\n",
      "Filling batch 21756~22012...\n",
      "Filling batch 22012~22268...\n",
      "Filling batch 22268~22524...\n",
      "Filling batch 22524~22780...\n",
      "Filling batch 22780~23036...\n",
      "Filling batch 23036~23292...\n",
      "Filling batch 23292~23548...\n",
      "Filling batch 23548~23804...\n",
      "Filling batch 23804~24060...\n",
      "Filling batch 24060~24316...\n",
      "Filling batch 24316~24572...\n",
      "Filling batch 24572~24828...\n",
      "Filling batch 24828~25084...\n",
      "Filling batch 25084~25340...\n",
      "Filling batch 25340~25596...\n",
      "Filling batch 25596~25852...\n",
      "Filling batch 25852~26104...\n",
      "Filling batch 26104~26360...\n",
      "Filling batch 26360~26616...\n",
      "Filling batch 26616~26872...\n",
      "Filling batch 26872~27128...\n",
      "Filling batch 27128~27384...\n",
      "Filling batch 27384~27640...\n",
      "Filling batch 27640~27896...\n",
      "Filling batch 27896~28152...\n",
      "Filling batch 28152~28408...\n",
      "Filling batch 28408~28664...\n",
      "Filling batch 28664~28920...\n",
      "Filling batch 28920~29176...\n",
      "Filling batch 29176~29432...\n",
      "Filling batch 29432~29688...\n",
      "Filling batch 29688~29944...\n",
      "Filling batch 29944~30200...\n",
      "Filling batch 30200~30456...\n",
      "Filling batch 30456~30712...\n",
      "Filling batch 30712~30968...\n",
      "Filling batch 30968~31224...\n",
      "Filling batch 31224~31480...\n",
      "Filling batch 31480~31736...\n",
      "Filling batch 31736~31992...\n",
      "Filling batch 31992~32248...\n",
      "Filling batch 32248~32504...\n",
      "Filling batch 32504~32760...\n",
      "Filling batch 32760~33016...\n",
      "Filling batch 33016~33272...\n",
      "Filling batch 33272~33528...\n",
      "Filling batch 33528~33784...\n",
      "Filling batch 33784~34040...\n",
      "Filling batch 34040~34296...\n",
      "Filling batch 34296~34552...\n",
      "Filling batch 34552~34808...\n",
      "Filling batch 34808~35064...\n",
      "Filling batch 35064~35320...\n",
      "Filling batch 35320~35576...\n",
      "Filling batch 35576~35832...\n",
      "Filling batch 35832~36088...\n",
      "Filling batch 36088~36344...\n",
      "Filling batch 36344~36600...\n",
      "Filling batch 36600~36856...\n",
      "Filling batch 36856~37112...\n",
      "Filling batch 37112~37368...\n",
      "Filling batch 37368~37624...\n",
      "Filling batch 37624~37880...\n",
      "Filling batch 37880~38136...\n",
      "Filling batch 38136~38392...\n",
      "Filling batch 38392~38648...\n",
      "Filling batch 38648~38904...\n",
      "Filling batch 38904~39156...\n",
      "Filling batch 39156~39412...\n",
      "Filling batch 39412~39668...\n",
      "Filling batch 39668~39924...\n",
      "Filling batch 39924~40180...\n",
      "Filling batch 40180~40436...\n",
      "Filling batch 40436~40692...\n",
      "Filling batch 40692~40948...\n",
      "Filling batch 40948~41204...\n",
      "Filling batch 41204~41460...\n",
      "Filling batch 41460~41716...\n",
      "Filling batch 41716~41972...\n",
      "Filling batch 41972~42228...\n",
      "Filling batch 42228~42484...\n",
      "Filling batch 42484~42740...\n",
      "Filling batch 42740~42996...\n",
      "Filling batch 42996~43252...\n",
      "Filling batch 43252~43508...\n",
      "Filling batch 43508~43764...\n",
      "Filling batch 43764~44020...\n",
      "Filling batch 44020~44276...\n",
      "Filling batch 44276~44532...\n",
      "Filling batch 44532~44788...\n",
      "Filling batch 44788~45044...\n",
      "Filling batch 45044~45300...\n",
      "Filling batch 45300~45556...\n",
      "Filling batch 45556~45812...\n",
      "Filling batch 45812~46068...\n",
      "Filling batch 46068~46324...\n",
      "Filling batch 46324~46580...\n",
      "Filling batch 46580~46836...\n",
      "Filling batch 46836~47092...\n",
      "Filling batch 47092~47348...\n",
      "Filling batch 47348~47604...\n",
      "Filling batch 47604~47860...\n",
      "Filling batch 47860~48116...\n",
      "Filling batch 48116~48372...\n",
      "Filling batch 48372~48628...\n",
      "Filling batch 48628~48884...\n",
      "Filling batch 48884~49140...\n",
      "Filling batch 49140~49396...\n",
      "Filling batch 49396~49652...\n",
      "Filling batch 49652~49908...\n",
      "Filling batch 49908~50164...\n",
      "Filling batch 50164~50420...\n",
      "Filling batch 50420~50676...\n",
      "Filling batch 50676~50932...\n",
      "Filling batch 50932~51188...\n",
      "Filling batch 51188~51444...\n",
      "Filling batch 51444~51700...\n",
      "Filling batch 51700~51956...\n",
      "Filling batch 51956~52208...\n",
      "Filling batch 52208~52464...\n",
      "Filling batch 52464~52720...\n",
      "Filling batch 52720~52976...\n",
      "Filling batch 52976~53232...\n",
      "Filling batch 53232~53488...\n",
      "Filling batch 53488~53744...\n",
      "Filling batch 53744~54000...\n",
      "Filling batch 54000~54256...\n",
      "Filling batch 54256~54512...\n",
      "Filling batch 54512~54768...\n",
      "Filling batch 54768~55024...\n",
      "Filling batch 55024~55280...\n",
      "Filling batch 55280~55536...\n",
      "Filling batch 55536~55792...\n",
      "Filling batch 55792~56048...\n",
      "Filling batch 56048~56304...\n",
      "Filling batch 56304~56560...\n",
      "Filling batch 56560~56816...\n",
      "Filling batch 56816~57072...\n",
      "Filling batch 57072~57328...\n",
      "Filling batch 57328~57584...\n",
      "Filling batch 57584~57840...\n",
      "Filling batch 57840~58096...\n",
      "Filling batch 58096~58352...\n",
      "Filling batch 58352~58608...\n",
      "Filling batch 58608~58864...\n",
      "Filling batch 58864~59120...\n",
      "Filling batch 59120~59376...\n",
      "Filling batch 59376~59632...\n",
      "Filling batch 59632~59888...\n",
      "Filling batch 59888~60144...\n",
      "Filling batch 60144~60400...\n",
      "Filling batch 60400~60656...\n",
      "Filling batch 60656~60912...\n",
      "Filling batch 60912~61168...\n",
      "Filling batch 61168~61424...\n",
      "Filling batch 61424~61680...\n",
      "Filling batch 61680~61936...\n",
      "Filling batch 61936~62192...\n",
      "Filling batch 62192~62448...\n",
      "Filling batch 62448~62704...\n",
      "Filling batch 62704~62960...\n",
      "Filling batch 62960~63216...\n",
      "Filling batch 63216~63472...\n",
      "Filling batch 63472~63728...\n",
      "Filling batch 63728~63984...\n",
      "Filling batch 63984~64240...\n",
      "Filling batch 64240~64496...\n",
      "Filling batch 64496~64752...\n",
      "Filling batch 64752~65008...\n",
      "Filling batch 65008~65260...\n",
      "sample count=2330\n",
      "Filling batch 0~256....\n",
      "Filling batch 256~512...\n",
      "Filling batch 512~768...\n",
      "Filling batch 768~1024...\n",
      "Filling batch 1024~1280...\n",
      "Filling batch 1280~1536...\n",
      "Filling batch 1536~1792...\n",
      "Filling batch 1792~2048...\n",
      "Filling batch 2048~2304...\n",
      "Filling batch 2304~2330...\n",
      "sample count=533\n",
      "Filling batch 0~256....\n",
      "Filling batch 256~512...\n",
      "Filling batch 512~533...\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = extract_features(df_resample, crop_dir, target_size, batch_size, 'train')\n",
    "validation_features, validation_labels = extract_features(df_resample, crop_dir, target_size, batch_size, 'validation')\n",
    "test_features, test_labels = extract_features(df_resample, crop_dir, target_size, batch_size, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    \"\"\"Save the model after every epoch.\n",
    "    `filepath` can contain named formatting options,\n",
    "    which will be filled the value of `epoch` and\n",
    "    keys in `logs` (passed in `on_epoch_end`).\n",
    "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
    "    then the model checkpoints will be saved with the epoch number and\n",
    "    the validation loss in the filename.\n",
    "    # Arguments\n",
    "        filepath: string, path to save the model file.\n",
    "        monitor: quantity to monitor.\n",
    "        verbose: verbosity mode, 0 or 1.\n",
    "        save_best_only: if `save_best_only=True`,\n",
    "            the latest best model according to\n",
    "            the quantity monitored will not be overwritten.\n",
    "        mode: one of {auto, min, max}.\n",
    "            If `save_best_only=True`, the decision\n",
    "            to overwrite the current save file is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc. In `auto` mode, the direction is\n",
    "            automatically inferred from the name of the monitored quantity.\n",
    "        save_weights_only: if True, then only the model's weights will be\n",
    "            saved (`model.save_weights(filepath)`), else the full model\n",
    "            is saved (`model.save(filepath)`).\n",
    "        period: Interval (number of epochs) between checkpoints.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath, monitor='val_loss', verbose=0,\n",
    "                 save_best_only=False, save_weights_only=False,\n",
    "                 mode='auto', period=1):\n",
    "        super(ModelCheckpoint, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.filepath = filepath\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.period = period\n",
    "        self.batches_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.batches_since_last_save += 1\n",
    "        if self.batches_since_last_save >= self.period:\n",
    "            self.batches_since_last_save = 0\n",
    "            filepath = self.filepath.format(epoch=batch + 1, **logs)\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    warnings.warn('Can save best model only with %s available, '\n",
    "                                  'skipping.' % (self.monitor), RuntimeWarning)\n",
    "                else:\n",
    "                    if self.monitor_op(current, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print('\\nBatch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                                  ' saving model to %s'\n",
    "                                  % (batch + 1, self.monitor, self.best,\n",
    "                                     current, filepath))\n",
    "                        self.best = current\n",
    "                        if self.save_weights_only:\n",
    "                            self.model.save_weights(filepath, overwrite=True)\n",
    "                        else:\n",
    "                            self.model.save(filepath, overwrite=True)\n",
    "                    else:\n",
    "                        if self.verbose > 0:\n",
    "                            print('\\nBatch %05d: %s did not improve from %0.5f' %\n",
    "                                  (batch + 1, self.monitor, self.best))\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('\\nBatch %05d: saving model to %s' % (batch + 1, filepath))\n",
    "                if self.save_weights_only:\n",
    "                    self.model.save_weights(filepath, overwrite=True)\n",
    "                else:\n",
    "                    self.model.save(filepath, overwrite=True)\n",
    "                    \n",
    "                    \n",
    "class CustomTensorBoard(TensorBoard):\n",
    "    \"\"\"TensorBoard basic visualizations.\n",
    "    [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n",
    "    is a visualization tool provided with TensorFlow.\n",
    "    This callback writes a log for TensorBoard, which allows\n",
    "    you to visualize dynamic graphs of your training and test\n",
    "    metrics, as well as activation histograms for the different\n",
    "    layers in your model.\n",
    "    If you have installed TensorFlow with pip, you should be able\n",
    "    to launch TensorBoard from the command line:\n",
    "    ```sh\n",
    "    tensorboard --logdir=/full_path_to_your_logs\n",
    "    ```\n",
    "    When using a backend other than TensorFlow, TensorBoard will still work\n",
    "    (if you have TensorFlow installed), but the only feature available will\n",
    "    be the display of the losses and metrics plots.\n",
    "    # Arguments\n",
    "        log_dir: the path of the directory where to save the log\n",
    "            files to be parsed by TensorBoard.\n",
    "        histogram_freq: frequency (in epochs) at which to compute activation\n",
    "            and weight histograms for the layers of the model. If set to 0,\n",
    "            histograms won't be computed. Validation data (or split) must be\n",
    "            specified for histogram visualizations.\n",
    "        write_graph: whether to visualize the graph in TensorBoard.\n",
    "            The log file can become quite large when\n",
    "            write_graph is set to True.\n",
    "        write_grads: whether to visualize gradient histograms in TensorBoard.\n",
    "            `histogram_freq` must be greater than 0.\n",
    "        batch_size: size of batch of inputs to feed to the network\n",
    "            for histograms computation.\n",
    "        write_images: whether to write model weights to visualize as\n",
    "            image in TensorBoard.\n",
    "        embeddings_freq: frequency (in epochs) at which selected embedding\n",
    "            layers will be saved. If set to 0, embeddings won't be computed.\n",
    "            Data to be visualized in TensorBoard's Embedding tab must be passed\n",
    "            as `embeddings_data`.\n",
    "        embeddings_layer_names: a list of names of layers to keep eye on. If\n",
    "            None or empty list all the embedding layer will be watched.\n",
    "        embeddings_metadata: a dictionary which maps layer name to a file name\n",
    "            in which metadata for this embedding layer is saved. See the\n",
    "            [details](https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)\n",
    "            about metadata files format. In case if the same metadata file is\n",
    "            used for all embedding layers, string can be passed.\n",
    "        embeddings_data: data to be embedded at layers specified in\n",
    "            `embeddings_layer_names`. Numpy array (if the model has a single\n",
    "            input) or list of Numpy arrays (if the model has multiple inputs).\n",
    "            Learn [more about embeddings](https://www.tensorflow.org/programmers_guide/embedding)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, log_dir='./logs',\n",
    "                 histogram_freq=0,\n",
    "                 batch_size=32,\n",
    "                 write_graph=True,\n",
    "                 write_grads=False,\n",
    "                 write_images=False,\n",
    "                 embeddings_freq=0,\n",
    "                 embeddings_layer_names=None,\n",
    "                 embeddings_metadata=None,\n",
    "                 embeddings_data=None):\n",
    "        super(TensorBoard, self).__init__()\n",
    "        global tf, projector\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            from tensorflow.contrib.tensorboard.plugins import projector\n",
    "        except ImportError:\n",
    "            raise ImportError('You need the TensorFlow module installed to use TensorBoard.')\n",
    "\n",
    "        self.log_dir = log_dir\n",
    "        self.histogram_freq = histogram_freq\n",
    "        self.merged = None\n",
    "        self.write_graph = write_graph\n",
    "        self.write_grads = write_grads\n",
    "        self.write_images = write_images\n",
    "        self.embeddings_freq = embeddings_freq\n",
    "        self.embeddings_layer_names = embeddings_layer_names\n",
    "        self.embeddings_metadata = embeddings_metadata or {}\n",
    "        self.batch_size = batch_size\n",
    "        self.embeddings_data = embeddings_data\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        if K.backend() == 'tensorflow':\n",
    "            self.sess = K.get_session()\n",
    "        if self.histogram_freq and self.merged is None:\n",
    "            for layer in self.model.layers:\n",
    "\n",
    "                for weight in layer.weights:\n",
    "                    mapped_weight_name = weight.name.replace(':', '_')\n",
    "                    tf.summary.histogram(mapped_weight_name, weight)\n",
    "                    if self.write_grads:\n",
    "                        grads = model.optimizer.get_gradients(model.total_loss,\n",
    "                                                              weight)\n",
    "\n",
    "                        def is_indexed_slices(grad):\n",
    "                            return type(grad).__name__ == 'IndexedSlices'\n",
    "                        grads = [\n",
    "                            grad.values if is_indexed_slices(grad) else grad\n",
    "                            for grad in grads]\n",
    "                        tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n",
    "\n",
    "                if hasattr(layer, 'output'):\n",
    "                    if isinstance(layer.output, list):\n",
    "                        for i, output in enumerate(layer.output):\n",
    "                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\n",
    "                    else:\n",
    "                        tf.summary.histogram('{}_out'.format(layer.name),\n",
    "                                             layer.output)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "        if self.write_graph:\n",
    "            self.writer = tf.summary.FileWriter(self.log_dir,\n",
    "                                                self.sess.graph)\n",
    "        else:\n",
    "            self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "        if self.embeddings_freq and self.embeddings_data is not None:\n",
    "            self.embeddings_data = standardize_input_data(self.embeddings_data, model.input_names)\n",
    "\n",
    "            embeddings_layer_names = self.embeddings_layer_names\n",
    "\n",
    "            if not embeddings_layer_names:\n",
    "                embeddings_layer_names = [layer.name for layer in self.model.layers\n",
    "                                          if type(layer).__name__ == 'Embedding']\n",
    "            self.assign_embeddings = []\n",
    "            embeddings_vars = {}\n",
    "\n",
    "            self.batch_id = batch_id = tf.placeholder(tf.int32)\n",
    "            self.step = step = tf.placeholder(tf.int32)\n",
    "\n",
    "            for layer in self.model.layers:\n",
    "                if layer.name in embeddings_layer_names:\n",
    "                    embedding_input = self.model.get_layer(layer.name).output\n",
    "                    embedding_size = np.prod(embedding_input.shape[1:])\n",
    "                    embedding_input = tf.reshape(embedding_input,\n",
    "                                                 (step, int(embedding_size)))\n",
    "                    shape = (self.embeddings_data[0].shape[0], int(embedding_size))\n",
    "                    embedding = tf.Variable(tf.zeros(shape),\n",
    "                                            name=layer.name + '_embedding')\n",
    "                    embeddings_vars[layer.name] = embedding\n",
    "                    batch = tf.assign(embedding[batch_id:batch_id + step],\n",
    "                                      embedding_input)\n",
    "                    self.assign_embeddings.append(batch)\n",
    "\n",
    "            self.saver = tf.train.Saver(list(embeddings_vars.values()))\n",
    "\n",
    "            embeddings_metadata = {}\n",
    "\n",
    "            if not isinstance(self.embeddings_metadata, str):\n",
    "                embeddings_metadata = self.embeddings_metadata\n",
    "            else:\n",
    "                embeddings_metadata = {layer_name: self.embeddings_metadata\n",
    "                                       for layer_name in embeddings_vars.keys()}\n",
    "\n",
    "            config = projector.ProjectorConfig()\n",
    "\n",
    "            for layer_name, tensor in embeddings_vars.items():\n",
    "                embedding = config.embeddings.add()\n",
    "                embedding.tensor_name = tensor.name\n",
    "\n",
    "                if layer_name in embeddings_metadata:\n",
    "                    embedding.metadata_path = embeddings_metadata[layer_name]\n",
    "\n",
    "            projector.visualize_embeddings(self.writer, config)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        epoch = batch\n",
    "        logs = logs or {}\n",
    "\n",
    "        if not self.validation_data and self.histogram_freq:\n",
    "            raise ValueError(\"If printing histograms, validation_data must be \"\n",
    "                             \"provided, and cannot be a generator.\")\n",
    "        if self.embeddings_data is None and self.embeddings_freq:\n",
    "            raise ValueError(\"To visualize embeddings, embeddings_data must \"\n",
    "                             \"be provided.\")\n",
    "        if self.validation_data and self.histogram_freq:\n",
    "            if epoch % self.histogram_freq == 0:\n",
    "\n",
    "                val_data = self.validation_data\n",
    "                tensors = (self.model.inputs +\n",
    "                           self.model.targets +\n",
    "                           self.model.sample_weights)\n",
    "\n",
    "                if self.model.uses_learning_phase:\n",
    "                    tensors += [K.learning_phase()]\n",
    "\n",
    "                assert len(val_data) == len(tensors)\n",
    "                val_size = val_data[0].shape[0]\n",
    "                i = 0\n",
    "                while i < val_size:\n",
    "                    step = min(self.batch_size, val_size - i)\n",
    "                    if self.model.uses_learning_phase:\n",
    "                        # do not slice the learning phase\n",
    "                        batch_val = [x[i:i + step] for x in val_data[:-1]]\n",
    "                        batch_val.append(val_data[-1])\n",
    "                    else:\n",
    "                        batch_val = [x[i:i + step] for x in val_data]\n",
    "                    assert len(batch_val) == len(tensors)\n",
    "                    feed_dict = dict(zip(tensors, batch_val))\n",
    "                    result = self.sess.run([self.merged], feed_dict=feed_dict)\n",
    "                    summary_str = result[0]\n",
    "                    self.writer.add_summary(summary_str, epoch)\n",
    "                    i += self.batch_size\n",
    "\n",
    "        for name, value in logs.items():\n",
    "            if name in ['batch', 'size']:\n",
    "                continue\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            if isinstance(value, np.ndarray):\n",
    "                summary_value.simple_value = value.item()\n",
    "            else:\n",
    "                summary_value.simple_value = value\n",
    "            summary_value.tag = name\n",
    "            self.writer.add_summary(summary, epoch)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def on_train_end(self, _):\n",
    "        self.writer.close()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_000.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_000.npy\n",
      "sample count=13052\n",
      "Processing image 4...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarvis/anaconda3/envs/naesa1.8/lib/python3.5/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/jarvis/anaconda3/envs/naesa1.8/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_001.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_001.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_002.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_002.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_003.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_003.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_004.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_004.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_005.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_005.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_006.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_006.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_007.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_007.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_008.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_008.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_009.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_009.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_010.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_010.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_011.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_011.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_012.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_012.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_013.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_013.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_014.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_014.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_015.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_015.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_016.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_016.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_017.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_017.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_018.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_018.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_019.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_019.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_020.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_020.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_021.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_021.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_022.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_022.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_023.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_023.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_024.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_024.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_025.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_025.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_026.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_026.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_027.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_027.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_028.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_028.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_029.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_029.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_030.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_030.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_031.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_031.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_032.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_032.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_033.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_033.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_034.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_034.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_035.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_035.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_036.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_036.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_037.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_037.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_038.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_038.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_039.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_039.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_040.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_040.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_041.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_041.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_042.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_042.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_043.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_043.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_044.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_044.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_045.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_045.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_046.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_046.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_047.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_047.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_048.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_048.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_049.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_049.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_050.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_050.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_051.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_051.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_052.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_052.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_053.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_053.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_054.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_054.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_055.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_055.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_056.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_056.npy\n",
      "sample count=13052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_057.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_057.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_058.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_058.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_059.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_059.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_060.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_060.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_061.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_061.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_062.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_062.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_063.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_063.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_064.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_064.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_065.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_065.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_066.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_066.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_067.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_067.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_068.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_068.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_069.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_069.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_070.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_070.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_071.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_071.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_072.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_072.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_073.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_073.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_074.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_074.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_075.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_075.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_076.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_076.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_077.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_077.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_078.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_078.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_079.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_079.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_080.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_080.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_081.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_081.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_082.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_082.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_083.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_083.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_084.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_084.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_085.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_085.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_086.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_086.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_087.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_087.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_088.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_088.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_089.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_089.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_090.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_090.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_091.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_091.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_092.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_092.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_093.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_093.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_094.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_094.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_095.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_095.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_096.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_096.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_097.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_097.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_098.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_098.npy\n",
      "sample count=13052\n",
      "/sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_features_099.npy /sdb1/share/ai_osteoporosis_hip/tmp/numpy/train_labels_099.npy\n",
      "sample count=13052\n",
      "Processing image 13052...\r"
     ]
    }
   ],
   "source": [
    "tmp_dir = os.path.join(share_dir, 'tmp/numpy')\n",
    "for i in range(100):\n",
    "    feature_file = '%s/train_features_%03d.npy' % (tmp_dir, i)\n",
    "    label_file = '%s/train_labels_%03d.npy' % (tmp_dir, i)\n",
    "    print(feature_file, label_file)\n",
    "    train_features, train_labels = extract_train_features(df_resample, crop_dir, target_size, batch_size)\n",
    "    np.save(feature_file, train_features)    \n",
    "    np.save(label_file, train_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65260 samples, validate on 2330 samples\n",
      "Epoch 202/300\n",
      "65260/65260 [==============================] - 2s 35us/step - loss: 0.5779 - acc: 0.6999 - val_loss: 0.5282 - val_acc: 0.7373\n",
      "Epoch 203/300\n",
      "65260/65260 [==============================] - 2s 31us/step - loss: 0.5204 - acc: 0.7413 - val_loss: 0.5256 - val_acc: 0.7335\n",
      "Epoch 204/300\n",
      "65260/65260 [==============================] - 2s 28us/step - loss: 0.5027 - acc: 0.7534 - val_loss: 0.3548 - val_acc: 0.8395\n",
      "Epoch 205/300\n",
      "65260/65260 [==============================] - 2s 27us/step - loss: 0.4898 - acc: 0.7630 - val_loss: 0.3458 - val_acc: 0.8425\n",
      "Epoch 206/300\n",
      "65260/65260 [==============================] - 2s 30us/step - loss: 0.4811 - acc: 0.7677 - val_loss: 0.3512 - val_acc: 0.8416\n",
      "Epoch 207/300\n",
      "17152/65260 [======>.......................] - ETA: 1s - loss: 0.4741 - acc: 0.7741"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-fb388e3256c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0;31m#validation_steps=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     validation_data=(validation_features, validation_labels))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/naesa1.8/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/naesa1.8/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/naesa1.8/lib/python3.5/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Dense(1024, activation='relu', input_dim=2048))\n",
    "model2.add(layers.Dropout(dropout_rate))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer=optimizers.RMSprop(learning_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "checkpoint_dir = os.path.join(work_dir, 'checkpoint/test')\n",
    "log_dir = os.path.join(work_dir, 'log/test')\n",
    "if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n",
    "model_path = os.path.join(checkpoint_dir, '{epoch:02d}-{val_acc:.4f}.hdf5')\n",
    "checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_acc', period=5)\n",
    "tensorboard = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "history = model2.fit(train_features, train_labels,\n",
    "                    epochs=300,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[checkpoint, tensorboard],\n",
    "                    #validation_steps=1,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_resample[df_resample['class2_1']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2330"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6047"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533/533 [==============================] - 0s 19us/step\n",
      "acc: 79.17%\n",
      "auc=0.873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "#df = get_clinical_data(csv_file)\n",
    "\n",
    "#test_file_list, img_test, clinical_test, y_test = get_image_data(test_dir, df)\n",
    "#test_file_list = []\n",
    "#for path, dirs, files in os.walk(test_dir):\n",
    "#    test_file_list += [os.path.join(path, file) for file in files]\n",
    "#count = len(test_file_list)\n",
    "\n",
    "#img_test = preprocess_input(img_test, is_train=False)\n",
    "y_pred = model2.predict(test_features)\n",
    "scores = model2.evaluate(test_features, test_labels, batch_size)\n",
    "\n",
    "#scores = model.evaluate_generator(custom_generator(test_file_list, df, is_train=False, non_image_data=True), steps=math.ceil(test_set_size/batch_size))\n",
    "print(\"%s: %.2f%%\" %(model2.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, y_pred)\n",
    "auc = auc(fpr, tpr)\n",
    "print('auc=%0.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.0%\n",
      "0, 67.3%\n",
      "0, 0.2%\n",
      "1, 0.1%\n",
      "0, 51.9%\n",
      "0, 0.5%\n",
      "0, 0.0%\n",
      "0, 0.1%\n",
      "0, 0.0%\n",
      "0, 0.4%\n",
      "0, 53.0%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.6%\n",
      "1, 79.2%\n",
      "0, 21.8%\n",
      "0, 0.1%\n",
      "0, 0.1%\n",
      "1, 98.8%\n",
      "0, 0.0%\n",
      "0, 3.4%\n",
      "0, 9.8%\n",
      "0, 0.0%\n",
      "0, 88.4%\n",
      "1, 69.2%\n",
      "0, 6.5%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "1, 18.5%\n",
      "0, 0.2%\n",
      "0, 51.2%\n",
      "0, 0.0%\n",
      "0, 57.4%\n",
      "0, 8.4%\n",
      "0, 0.0%\n",
      "0, 97.1%\n",
      "0, 6.7%\n",
      "0, 0.6%\n",
      "0, 0.0%\n",
      "0, 13.6%\n",
      "0, 98.1%\n",
      "0, 67.4%\n",
      "0, 0.0%\n",
      "0, 86.2%\n",
      "0, 0.1%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "1, 95.3%\n",
      "0, 0.0%\n",
      "0, 0.3%\n",
      "0, 0.0%\n",
      "1, 16.9%\n",
      "0, 0.3%\n",
      "0, 2.9%\n",
      "1, 99.9%\n",
      "0, 54.5%\n",
      "0, 59.7%\n",
      "1, 99.7%\n",
      "0, 0.2%\n",
      "1, 10.9%\n",
      "0, 5.3%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.2%\n",
      "1, 64.8%\n",
      "0, 0.0%\n",
      "0, 0.3%\n",
      "0, 0.0%\n",
      "0, 49.6%\n",
      "0, 11.0%\n",
      "0, 0.0%\n",
      "0, 2.8%\n",
      "0, 98.3%\n",
      "0, 8.5%\n",
      "0, 0.0%\n",
      "0, 3.2%\n",
      "0, 98.6%\n",
      "0, 3.6%\n",
      "0, 24.6%\n",
      "1, 89.3%\n",
      "0, 0.0%\n",
      "0, 21.2%\n",
      "0, 10.5%\n",
      "0, 2.4%\n",
      "0, 2.1%\n",
      "0, 1.9%\n",
      "0, 9.2%\n",
      "0, 73.6%\n",
      "0, 77.4%\n",
      "0, 87.1%\n",
      "0, 2.2%\n",
      "0, 0.1%\n",
      "0, 0.0%\n",
      "0, 1.6%\n",
      "0, 0.9%\n",
      "0, 3.3%\n",
      "0, 83.2%\n",
      "0, 0.0%\n",
      "0, 75.7%\n",
      "0, 2.1%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 55.7%\n",
      "0, 1.6%\n",
      "1, 61.4%\n",
      "0, 0.0%\n",
      "0, 4.2%\n",
      "0, 0.4%\n",
      "0, 28.4%\n",
      "0, 23.2%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 91.2%\n",
      "0, 0.1%\n",
      "0, 5.5%\n",
      "0, 99.9%\n",
      "0, 1.2%\n",
      "0, 0.0%\n",
      "1, 93.0%\n",
      "0, 57.7%\n",
      "1, 99.8%\n",
      "0, 0.0%\n",
      "0, 98.7%\n",
      "0, 0.0%\n",
      "0, 0.3%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 3.1%\n",
      "0, 58.5%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.3%\n",
      "0, 0.1%\n",
      "0, 4.7%\n",
      "0, 0.0%\n",
      "0, 6.7%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 34.8%\n",
      "0, 0.0%\n",
      "0, 8.7%\n",
      "0, 90.9%\n",
      "0, 97.5%\n",
      "0, 94.5%\n",
      "0, 0.1%\n",
      "0, 1.4%\n",
      "0, 0.7%\n",
      "0, 92.4%\n",
      "0, 0.2%\n",
      "0, 95.8%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 100.0%\n",
      "0, 0.0%\n",
      "0, 20.1%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 12.4%\n",
      "0, 0.6%\n",
      "0, 81.6%\n",
      "0, 0.0%\n",
      "1, 99.3%\n",
      "0, 36.1%\n",
      "0, 26.9%\n",
      "0, 1.3%\n",
      "0, 38.4%\n",
      "0, 1.7%\n",
      "0, 88.8%\n",
      "0, 5.4%\n",
      "0, 0.0%\n",
      "0, 0.6%\n",
      "0, 10.8%\n",
      "0, 0.1%\n",
      "0, 39.6%\n",
      "1, 43.2%\n",
      "1, 100.0%\n",
      "0, 0.1%\n",
      "0, 0.1%\n",
      "0, 0.2%\n",
      "0, 0.1%\n",
      "0, 0.1%\n",
      "0, 0.1%\n",
      "1, 100.0%\n",
      "0, 0.0%\n",
      "1, 99.5%\n",
      "1, 100.0%\n",
      "0, 18.5%\n",
      "0, 0.0%\n",
      "1, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "1, 99.0%\n",
      "0, 13.7%\n",
      "0, 0.4%\n",
      "0, 0.0%\n",
      "0, 78.9%\n",
      "0, 19.3%\n",
      "0, 0.0%\n",
      "0, 7.2%\n",
      "0, 0.0%\n",
      "0, 1.2%\n",
      "0, 0.1%\n",
      "0, 33.5%\n",
      "0, 88.7%\n",
      "1, 83.3%\n",
      "0, 1.6%\n",
      "0, 10.3%\n",
      "0, 54.0%\n",
      "0, 0.4%\n",
      "0, 0.0%\n",
      "1, 95.6%\n",
      "0, 45.7%\n",
      "0, 0.6%\n",
      "0, 19.8%\n",
      "0, 0.0%\n",
      "0, 97.2%\n",
      "0, 0.1%\n",
      "1, 12.4%\n",
      "1, 100.0%\n",
      "1, 98.6%\n",
      "0, 98.4%\n",
      "0, 18.6%\n",
      "1, 94.6%\n",
      "0, 0.0%\n",
      "0, 84.2%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 2.8%\n",
      "1, 100.0%\n",
      "0, 6.5%\n",
      "0, 0.0%\n",
      "0, 63.8%\n",
      "0, 2.1%\n",
      "0, 0.0%\n",
      "0, 2.2%\n",
      "0, 19.7%\n",
      "0, 0.3%\n",
      "1, 66.5%\n",
      "0, 0.0%\n",
      "0, 83.5%\n",
      "0, 1.3%\n",
      "0, 0.0%\n",
      "0, 1.7%\n",
      "0, 42.5%\n",
      "0, 0.0%\n",
      "0, 92.6%\n",
      "0, 0.0%\n",
      "0, 0.4%\n",
      "0, 0.0%\n",
      "0, 78.6%\n",
      "0, 0.1%\n",
      "0, 0.1%\n",
      "0, 86.5%\n",
      "0, 3.6%\n",
      "0, 91.1%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 98.2%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.1%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.9%\n",
      "0, 27.1%\n",
      "0, 47.5%\n",
      "1, 99.8%\n",
      "1, 69.1%\n",
      "1, 99.5%\n",
      "0, 0.1%\n",
      "0, 0.0%\n",
      "1, 100.0%\n",
      "0, 0.3%\n",
      "0, 93.1%\n",
      "1, 99.9%\n",
      "0, 69.4%\n",
      "0, 10.3%\n",
      "0, 2.8%\n",
      "1, 88.1%\n",
      "0, 4.9%\n",
      "0, 0.0%\n",
      "0, 0.1%\n",
      "0, 97.6%\n",
      "0, 99.7%\n",
      "0, 0.0%\n",
      "0, 0.2%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 40.0%\n",
      "0, 5.7%\n",
      "0, 26.9%\n",
      "0, 0.0%\n",
      "0, 37.1%\n",
      "0, 0.0%\n",
      "0, 0.1%\n",
      "1, 13.8%\n",
      "0, 0.0%\n",
      "0, 0.1%\n",
      "0, 3.3%\n",
      "0, 0.8%\n",
      "0, 4.2%\n",
      "0, 19.3%\n",
      "0, 79.7%\n",
      "0, 9.8%\n",
      "0, 2.8%\n",
      "0, 51.8%\n",
      "0, 99.9%\n",
      "1, 29.5%\n",
      "1, 78.8%\n",
      "0, 0.0%\n",
      "1, 55.3%\n",
      "0, 0.2%\n",
      "0, 0.0%\n",
      "0, 48.3%\n",
      "0, 0.0%\n",
      "0, 2.1%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 93.9%\n",
      "0, 9.5%\n",
      "0, 0.0%\n",
      "1, 100.0%\n",
      "0, 99.4%\n",
      "0, 1.1%\n",
      "0, 1.0%\n",
      "0, 0.0%\n",
      "0, 8.0%\n",
      "0, 14.1%\n",
      "0, 0.0%\n",
      "1, 99.2%\n",
      "0, 0.0%\n",
      "1, 13.0%\n",
      "0, 0.1%\n",
      "0, 85.6%\n",
      "0, 84.2%\n",
      "0, 100.0%\n",
      "0, 0.2%\n",
      "0, 0.0%\n",
      "1, 89.1%\n",
      "1, 94.5%\n",
      "0, 97.1%\n",
      "0, 91.8%\n",
      "0, 80.1%\n",
      "0, 0.0%\n",
      "0, 0.7%\n",
      "0, 0.0%\n",
      "0, 0.4%\n",
      "0, 99.2%\n",
      "0, 99.0%\n",
      "0, 2.5%\n",
      "0, 37.2%\n",
      "0, 71.5%\n",
      "0, 0.5%\n",
      "0, 0.0%\n",
      "0, 40.5%\n",
      "0, 0.2%\n",
      "0, 4.7%\n",
      "1, 1.2%\n",
      "1, 89.8%\n",
      "0, 91.1%\n",
      "0, 84.3%\n",
      "0, 0.1%\n",
      "1, 79.8%\n",
      "0, 0.0%\n",
      "0, 33.5%\n",
      "0, 0.1%\n",
      "0, 0.0%\n",
      "0, 0.9%\n",
      "0, 99.8%\n",
      "0, 99.3%\n",
      "0, 0.0%\n",
      "0, 30.0%\n",
      "1, 100.0%\n",
      "0, 0.2%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.1%\n",
      "0, 68.5%\n",
      "0, 4.4%\n",
      "0, 0.0%\n",
      "0, 2.9%\n",
      "1, 11.2%\n",
      "1, 99.2%\n",
      "0, 0.0%\n",
      "0, 29.3%\n",
      "0, 22.7%\n",
      "0, 0.1%\n",
      "0, 0.7%\n",
      "0, 25.4%\n",
      "0, 0.5%\n",
      "0, 2.3%\n",
      "1, 94.8%\n",
      "0, 21.7%\n",
      "0, 1.6%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 26.6%\n",
      "0, 0.1%\n",
      "0, 61.4%\n",
      "0, 64.6%\n",
      "0, 1.8%\n",
      "0, 0.8%\n",
      "0, 0.6%\n",
      "0, 90.0%\n",
      "0, 37.3%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 86.3%\n",
      "0, 0.0%\n",
      "0, 69.6%\n",
      "0, 0.0%\n",
      "0, 71.7%\n",
      "0, 0.8%\n",
      "0, 17.0%\n",
      "0, 0.0%\n",
      "0, 51.3%\n",
      "0, 11.9%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 92.8%\n",
      "1, 92.3%\n",
      "0, 85.6%\n",
      "0, 0.0%\n",
      "0, 0.3%\n",
      "0, 0.1%\n",
      "0, 16.8%\n",
      "0, 92.6%\n",
      "0, 2.6%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 2.2%\n",
      "0, 35.9%\n",
      "1, 99.3%\n",
      "0, 0.0%\n",
      "0, 0.1%\n",
      "1, 99.7%\n",
      "0, 55.3%\n",
      "0, 0.0%\n",
      "0, 69.7%\n",
      "0, 1.5%\n",
      "1, 87.0%\n",
      "0, 32.0%\n",
      "0, 8.1%\n",
      "0, 0.1%\n",
      "0, 0.8%\n",
      "1, 98.6%\n",
      "0, 80.7%\n",
      "0, 0.0%\n",
      "0, 2.5%\n",
      "1, 99.8%\n",
      "0, 99.8%\n",
      "0, 68.3%\n",
      "0, 49.7%\n",
      "0, 0.2%\n",
      "0, 2.1%\n",
      "0, 30.8%\n",
      "1, 25.3%\n",
      "0, 2.8%\n",
      "0, 0.1%\n",
      "0, 0.0%\n",
      "0, 62.0%\n",
      "0, 1.5%\n",
      "0, 0.0%\n",
      "0, 91.7%\n",
      "1, 93.3%\n",
      "0, 0.0%\n",
      "0, 1.4%\n",
      "0, 11.4%\n",
      "1, 99.7%\n",
      "0, 93.6%\n",
      "0, 0.3%\n",
      "0, 50.8%\n",
      "1, 99.8%\n",
      "1, 99.8%\n",
      "0, 0.1%\n",
      "0, 17.9%\n",
      "1, 4.8%\n",
      "0, 10.0%\n",
      "0, 6.7%\n",
      "0, 98.1%\n",
      "1, 100.0%\n",
      "0, 0.0%\n",
      "0, 99.2%\n",
      "0, 0.8%\n",
      "0, 0.0%\n",
      "1, 28.5%\n",
      "0, 0.0%\n",
      "1, 98.5%\n",
      "1, 96.8%\n",
      "0, 89.1%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.2%\n",
      "0, 0.0%\n",
      "0, 70.7%\n",
      "0, 3.0%\n",
      "0, 61.9%\n",
      "1, 100.0%\n",
      "1, 99.4%\n",
      "0, 0.0%\n",
      "0, 31.2%\n",
      "0, 0.0%\n",
      "0, 9.0%\n",
      "0, 5.0%\n",
      "0, 7.6%\n",
      "0, 0.0%\n",
      "1, 100.0%\n",
      "0, 42.9%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 0.4%\n",
      "0, 1.0%\n",
      "0, 11.1%\n",
      "1, 0.9%\n",
      "0, 33.8%\n",
      "0, 2.2%\n",
      "0, 0.2%\n",
      "1, 7.2%\n",
      "0, 0.4%\n",
      "0, 2.3%\n",
      "0, 2.8%\n",
      "0, 0.2%\n",
      "0, 0.1%\n",
      "0, 99.6%\n",
      "0, 0.0%\n",
      "0, 0.0%\n",
      "0, 1.5%\n",
      "0, 0.9%\n",
      "0, 0.8%\n",
      "74 533\n"
     ]
    }
   ],
   "source": [
    "porosis = 0\n",
    "total = 0\n",
    "for label, pred in zip(test_labels, y_pred):\n",
    "    if label == 1: porosis += 1\n",
    "    total+=1\n",
    "    print('%d, %0.1f%%' % (label, pred*100))\n",
    "print(porosis, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65260, 2048)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_dir = os.path.join(share_dir, 'tmp')\n",
    "outfile = os.path.join(tmp_dir, 'train_features.npy')\n",
    "np.save(outfile, train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features2 = np.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65260, 2048)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
